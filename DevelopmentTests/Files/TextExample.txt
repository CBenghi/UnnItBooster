Abstract Automation has become an increasingly prominent feature of use of digital technologies in financial institutions, particularly in relation to how projects are monitored and controlled. While existing research frequently emphasises efficiency and performance improvements, there remains limited integrated understanding of how automation reshapes governance structures and professional roles within project environments. This dissertation examines how automation influences project monitoring and control in financial institutions, with attention to process performance, structural governance, and organisational and workforce implications. The study adopts a secondary research design based on a systematic literature review combined with scientometric analysis. Using PRISMA guidelines, 114 academic and institutional publications were analysed to identify thematic patterns, research clusters, and trends within the literature. The findings were interpreted through a socio-technical lens, drawing on Business Process Management, governance theory, and automation and work perspectives. The analysis indicates that automation alters the logic of project monitoring and control by enabling more continuous and predictive forms of oversight. At the process level, automation supports earlier visibility of potential risks and deviations, although these benefits are highly dependent on organisational context and decision authority. At the structural level, automation reshapes governance by embedding rules embedded in systems within technical systems, creating challenges related to accountability, transparency, and regulatory scrutiny. The study also finds that automation significantly affects workforce roles, shifting project professionals toward supervisory and interpretive responsibilities rather than eliminating human involvement. Overall, the findings suggest that automation should be understood as a socio-technical transformation rather than a purely technical enhancement. Automation amplifies existing organisational conditions and redistributes control rather than replacing it. The study concludes that sustainable automation in project monitoring and control requires alignment between technical capability, governance frameworks, and human oversight, particularly within the regulated and risk-sensitive context of financial institutions. Keywords: Automation, Project Monitoring and Control, Financial Institutions, Predictive Analytics, Algorithmic Governance, Workforce Transformation 1 Introduction 
1.1 Background Automation has become increasingly entrenched in the operations of financial institutions, influencing how work is organised, decisions are made, and projects are controlled. The adoption of automation in financial institutions has evolved from simple mechanisation of clerical tasks to advanced AI-guided decision-making tools. Early automation focused on transaction processing and ledger maintenance, while modern applications extend to credit risk modelling, fraud detection, and regulatory reporting (Davenport & Ronanki, 2018). This evolution reflects a shift in how automation is positioned within organisations, moving beyond back-office support toward systems that influence planning and control decisions. Financial institutions are usually characterised by a hierarchical structure, paper-based processes and strict regulations, are adopting digital technology to improve its productivity, customer service, and decision making. However, the rise of digital technologies has disrupted this model. According to the Deloitte 2024 Digital Banking Maturity Report, over 70% of global financial institutions have increased their digital investment budgets since 2020, and more than half now deploy automation to enhance operational efficiency and customer experience. Automation technologies, in particular process automation and intelligent workflow systems, are determining how monitoring and control are conducted in financial institutions. Tools like process automation and intelligent workflow systems can automate repetitive tasks, minimise human error, and enable predictive analytics for resource planning and risk forecasting. In this environment there is a discipline of project monitoring and control, which ensures that automation-driven initiatives are executed effectively, in line of the scope and budget, and also aligned with the objectives of the company. Predictive analytics, machine learning algorithms, and intelligent workflow systems enable managers to forecast risks, simulate project scenarios, and optimize resource allocation with higher precision (Mikalef et al., 2019; Lacity & Willcocks, 2021). However, notwithstanding these advantages, automation introduces new challenges that can impact project monitoring and control. Algorithm guided decision making can create challenges for clarity and responsibility, as the output may not be easily interpretable by human managers. Ethical consideration has also become a main point in automated project control. Decisions made by AI or machine learning models may introduce biases or obscure accountability if oversight frameworks are insufficient. Effective governance frameworks may include model validation, explainability, and ethical oversight which are essential to reduce the potential for risks of biased or untraceable decisions (McKinsey, 2024; Konigstorfer & Thalmann, 2023). Some of these risks include data history, data collections, difficulties with system integration, and third-party governance risks (Afanasiev & Kandinskaia, 2021). The increasing reliance on automated system has also caused worries around accountability and reliability, particularly when monitoring outputs that are derived from an algorithm rather than human supervision (Konigstorfer & Thalmann, 2023). The rationale for the research comes from the relationship between automation, and project monitoring and control within financial institutions. Automation should be understood as a socio-technical transformation as it affects processes, governance, and human roles simultaneously. 
1.2 Problem Statement Automation is changing the operational and managerial landscape of financial institutions, which is leading to greater efficiency, compliance and data-guided decision-making. However, this has also introduced a complex array of difficulties that directly affect the function of project monitoring and control. Automation tools enhance visibility, accuracy, and real-time oversight of project and they also create a dependency on algorithmic systems whose performance and decision rules are often difficult to validate (Konigstorfer & Thalmann, 2023; Afanasiev & Kandinskaia, 2021). More recently, a rising issue has emerged around the social and organisational consequences of automation in the financial sector. While automation improves accuracy and operational efficiency, it has also triggered widespread workforce restructuring and displacements, as some monitoring and control task once handled by project teams are now managed by automated systems. According to the Future of Jobs Reports by World Economic Forum (2023), automation and AI are expected to displace approximately 83 million jobs globally by 2027, with financial services being among the most affected industries due to the high potential for task automation. Similarly, a BankSETA (2024) study on The Future of Work and New Mutual Influence & Constraint Project Control Effectiveness (Anticipatory, Accountable & Human Governed) Figure 21: Conceptual Framework - Automation in Project Monitoring & Control Models for Skills Development in the Banking Sector found that automation is changing job roles, demanding new digital and analytical skills, and increasing employee resistance to technological change. This growing dependence on automation has consequently created a governance dilemma, which is how to ensure accountability, ethical oversight, and balanced integration of automated systems without ruining human judgement and institutional trust. As Susskind (2022) argue, automation can diminish professional discretion and weaken human oversight if governance frameworks are not adapted to sustain transparency and shared accountability. Therefore, this research seeks to examine how automation influences project monitoring and control in financial institutions, investigating not only the technical impact on accuracy and effectiveness but also its organisational implications. 
1.3 Aim and Objectives The main aim of this project is to examine the influence of automation on project monitoring and control in financial institutions, with a focus on how automation affects project planning, execution, and decision-making accuracy. Specifically, the study seeks to understand how automation enhances performance monitoring, compliance, and risk management, while how it reshapes workforce structures and organisational accountability. To achieve the aim of understanding how automation influences project monitoring and control in financial institutions, the study will pursue the following objectives: i. To examine the current practices, processes, and systems used for project monitoring and control within financial institutions. ii. To analyse how automation is applied within financial institutions to strengthen project monitoring and control processes. iii. To analyse how automation influences the effectiveness, governance and risk profile of project monitoring and control. iv. To examine the organisational and workforce implication of automation in project monitoring and control and propose strategies for sustainable implementation. 
1.4 Research Questions The following research questions guide the study: i. What are the current practices, processes, and systems used for project monitoring and control within financial institutions? ii. How is automation being applied within financial institutions to support and enhance project monitoring and control activities? iii. In what ways does automation influence the effectiveness, accuracy, and risk profile of project monitoring and control? iv. What organisational and workforce implications arise from the adoption of automation in project monitoring and control, and what strategies can support sustainable implementation? 
1.5 Significance of the Study The study is significant because it addresses a critical and timely gap in understanding how automation is transforming project monitoring and control within financial institutions. As the sector increasingly relies on automated systems to manage operational and regulatory demands, there is a growing need to evaluate whether these technologies genuinely improve monitoring accuracy, decisionmaking, and project outcomes, or whether they introduce new risks and organisational challenges. From a practical standpoint, this research is valuable to project managers, risk analysts, operations leaders, and use of digital technologies teams. By examining the influence of automation on performance, governance, and workforce dynamics, the study supports more informed decision-making regarding the adoption, integration, and oversight of automation tools. By identifying the organisational and humancentred implications, it provides guidance for designing balanced monitoring frameworks that maintain accountability and employee engagement while leveraging technological efficiencies. At a strategic level, the findings will assist financial institutions in developing sustainable automation roadmaps that align with long-term organisational goals. The study’s recommendations will offer a direction for mitigating risks such as overreliance on automated controls, skill gaps, and resistance to change, ensuring that automation contributes positively to operational stability rather than undermining it. Overall, the study brings together technical, organisational, and governance perspectives to examine how automation influences project monitoring and control in practice. 
1.6 Research Process This project will be completed in several phases to ensure there is progress in achieving the aim of the research, which is to examine how automation influences project monitoring and control in financial institutions. Phase 1: Project Definition and Planning • Finalise the research topic, scope, and objectives. • Conduct a review to identify key themes and relevant databases. • Develop a detailed research plan. Phase 2: Literature Review and Theoretical Development • Conduct an in-depth literature review focusing on automation, project control mechanisms, and financial sector. • Identify theoretical frameworks (Socio-Technical Systems Theory, Dynamic Capabilities Theory). Phase 3: Data Collection and Organisation • Gather secondary data from credible sources (academic journals, consultancy reports, and institutional publications from 2020–2025). • Organise literature and data thematically for coding and synthesis. Phase 4: Data Analysis • Apply a themed based analysis to identify patterns, correlations, and trends. • Interpret findings within the context of the theoretical models. Phase 5: Discussion • Compare findings against existing studies. • Evaluate the implications for project monitoring and control within financial institutions. Phase 6: Conclusion, Recommendations, and Final Report • Summarise key findings, draw conclusions, and formulate recommendations for improving project control through use of digital technologies. • Finalise dissertation and submit. 
1.7 Chapter Breakdown Chapter One provides the foundation for the study by introducing the context in which the research is situated and outlining the key elements that shape the investigation. It begins with a comprehensive background to the study, which situates automation within the larger financial sector and highlights its growing influence on project monitoring and control. These developments help explain why automation in project monitoring and control has become a relevant issue for financial institutions. The chapter then presents the problem statement, where the core challenges, tensions, and gaps associated with automation in project environments, particularly within financial institutions are clearly identified. Following this, the chapter outlines the aim of the study and the specific objectives that guide the research direction. These objectives form the basis for the research questions, which are also detailed in this chapter to provide clarity on what the study seeks to answer. The significance of the study is then discussed, explaining the practical and academic value of the research, and demonstrating how it contributes to the understanding of automation and project control practices. The chapter also delineates the scope of the study, specifying its boundaries, focus areas, and limitations to ensure clarity and alignment with the research problem. Finally, Chapter One concludes by presenting the structure of the dissertation, offering a brief overview of the chapters that follow and showing how each contributes to the development of the study. This creates a coherent roadmap that guides the reader through the research process from beginning to end. 
1.8 Chapter Summary Chapter One outlines the context for the study, focusing on the growing role of automation in financial institutions and the challenges it introduces for project monitoring and control. While automation enhances efficiency, accuracy, real-time oversight, and risk management, it also presents challenges such as governance complexities, workforce restructuring, and reliance on algorithmic decision-making. The study aims to examine how automation affects project planning, execution, and decision-making, with objectives focusing on current practices, automation applications, effectiveness, and workforce implications. The chapter highlights the significance of understanding these dynamics for both academic knowledge and practical guidance, supporting sustainable and accountable automation adoption. Finally, it outlines the research process and dissertation structure, providing a clear roadmap for investigating the influence of automation on project monitoring and control. 2 Methodology 
2.1 Research Design This research is based on secondary quantitative data and draws on published academic, institutional, and industry sources to examine automation in project monitoring and control. The design will enable an analysis of how automation adoption affects project monitoring control performance, governance, and workforce outcomes without primary data collection. Similar secondary research approaches have been used in prior studies of use of digital technologies where access to organisational data is limited. The study will be designed to align with the four objectives. i. Automation adoption drivers: Using quantitative data from industry reports, publications and institutional records, the study will evaluate the extent, nature and drivers of automation adoption in project monitoring and control. ii. Application of automation within financial institutions to strengthen project monitoring and control processes iii. Effectiveness, governance, and risk: By analysing performance dashboards, project reports, and archival datasets, the study will assess the influence of automation on project outcomes, governance structures, and risk management. iv. Organisational and workforce implications: Through an analysis of secondary literature and institutional reports, the study will explore workforce impacts, role redesign, and skill requirements associated with automation adoption. 
2.2 Search Strategy To obtain comprehensive bibliometric data relevant to the focus of the study, Scopus, Dimensions, Lens and Web of Science were selected as the primary research databases. These databases were selected due to their coverage of peer-reviewed research in information systems, project management, and financial services. To streamline the data retrieval process, a structured and systematic keyword strategy was developed to capture all relevant concepts associated with automation and project monitoring and control within financial institutions. A broad list of primary and alternative keywords was identified, covering the central constructs of automation, financial institutions, and project monitoring and control. For the automation category, keywords such as “automation,” “use of digital technologies,” “process automation,” “rule-based and predictive systems,” “artificial intelligence,” “process automation,” and “workflow automation” were used. For the project monitoring and control dimension, keywords included “project monitoring,” “project control,” “project governance,” “risk management,” “performance monitoring,” and “project oversight.” To capture the financial institutional context, keywords such as “financial institutions,” “banking sector,” “financial services,” “fintech,” and “digital banking” were incorporated. 
2.3 Selection of Studies The literature review followed PRISMA guidelines to support a transparent and replicable selection process. This approach allows for the identification, screening, eligibility assessment, and inclusion of relevant studies examining the impact of global recessions on project budgeting and cost management across industries. Identification: A comprehensive search was conducted across multiple academic databases, including (Web of Science, Dimensions, Scopus and Lens), yielding a total of 427 records. An additional 53 records were identified through other sources such as reference lists, and organisational reports published between 2012 and 2025. Following the initial search, 251 duplicate records were removed, resulting in 176 unique records for screening. Screening: The titles and abstracts of the 176 unique records were screened for relevance to the study objectives. During this stage, 27 studies were excluded due to irrelevance, leaving 149 full-text articles for detailed eligibility assessment. Eligibility: The full texts of the remaining 149 articles were thoroughly reviewed against pre-defined inclusion criteria, which focused on studies addressing automation, project monitoring and control. Studies were excluded if they did not directly address the research objectives, were not empirical or theoretical studies. After applying these criteria, 35 studies were excluded. Inclusion: Ultimately, 114 studies met all eligibility criteria and were included in the systematic review. These studies formed the evidence base for analysing the influence of automation on project monitoring and control in financial institutions. The PRISMA flow diagram summarising the identification, screening, eligibility assessment, and inclusion process is presented in Figure (1). Figure 1: PRISMA flow diagram for evaluation and screening of retrieved articles from database search. 
2.4 Data Quality Secondary data was collected from the following sources: • Institutional project reports and dashboards: Financial institution records will provide data on project timelines, budget adherence, cost, schedule, and risk indicators, and automation initiatives. • Industry and consulting reports: Publications from Deloitte, McKinsey, PwC, and other credible consulting firms will provide sector-wide understandings into automation adoption, productivity improvements, and workforce changes. • Regulatory publications and databases: Central banks, financial regulatory authorities, and sector-specific reports (e.g., BankSETA, World Economic Forum) will offer quantitative data on automation adoption trends and workforce implications. • Academic and peer-reviewed literature: Studies on automation in project management, financial institutions, and use of digital technologies will provide validated findings and theoretical underpinnings. 
2.5 Data Extraction and Bibliometric Analysis Co-occurrence Network of Keywords This focuses on the co-occurrence of keywords which focuses on the research subject and its relationship over the analysed period. This section interprets the network by positioning "automation, and project monitoring and control” as the central point of inquiry. Figure 2, generated using VOS Viewer software, highlights the results of bibliometric data. Among the 1381 keywords found in the 114 studies, 76 keywords met the criteria. The result is depicted in Figure 2 using network visualisation. Figure 2: Co-occurrence of keywords  3 Results 
3.1 Scientometric Analysis The Scientometric analysis provides an overview of the research concerning automation and project monitoring and control within financial institutions. This analysis focuses on identifying patterns, dominant themes, and linkages complementing the systematic review conducted in accordance with PRISMA guidelines. A total of 114 studies were included in the analysis, sourced from four major academic databases: Scopus, Web of Science, Dimensions, and Lens. Keyword data extracted from these studies yielded 1,381 terms, of which 76 keywords met the threshold for inclusion in the co-occurrence network. The analysis was performed using VOS Viewer, which enabled visualisation of the relationships and interconnections between the key concepts. The co-occurrence network revealed three primary clusters that reflect the multidimensional nature of research in this domain: i. Structural Performance Cluster: This cluster emphasises system architecture, Automation in Project Monitoring and Control Process Performance (Efficiency & Predictive Control) Structural Governance (Accountability, Transparency & Risks) Workforce Dynamics (Judgement, Trust & Roles) process integration challenges, quality of data, and the reliability of automation systems (van der Aalst et al., 2018; Afanasiev & Kandinskaia, 2021). It highlights the technical and infrastructural considerations critical to successful automation adoption in project monitoring. ii. Process Performance Cluster: Focused on real-time monitoring, predictive analytics, continuous auditing, and automated reporting, this cluster draws attention to the operational benefits of automation, including improved accuracy, timeliness, transparency, and risk prediction (Lacity & Willcocks, 2021; Basak et al., 2022; Mikalef et al., 2019). iii. Organisational and Workforce Implications Cluster: This cluster captures the human-centric aspects of automation adoption, including workforce role redesign, skill requirements, resistance to technological change, and organisational change management strategies (McKinsey, 2023; PwC, 2023). It highlights that technological efficiency alone is insufficient without aligning workforce capabilities and managing employee adaptation. 
3.1.1 Citation Analysis Overview of Citation Influence Across the Dataset The citation landscape across the 114 publications reviewed in the study presents a highly asymmetric distribution, characteristic of scientometric patterns described in classic bibliometric research. Much like the well-known power-law or long-tail distributions observed in large scientific corpora, a small number of publications in this dataset account for a disproportionately large share of overall citations. These highly cited works, representing approximately 18 percent of the total dataset form the foundational intellectual structure of the fields of process mining, automation in financial monitoring, use of digital technologies, and human–AI collaboration. In comparison, a large cluster of mid-tier papers (approximately 23 percent of the dataset) exhibit moderate but meaningful citation impact. These works typically refine, extend, or contextualise foundational theories and computational methods. Most publications (around 59 percent) fall into Tier 3, Tier 4, or Tier 5 categories, consisting of emerging research, recent papers that have not yet had time to accumulate citations, and non-indexed but practically influential grey literature such as regulatory guidelines, consultancy reports, and technical standards. Figure 3: Citation Tiers 1 - 5 Citation tiers show that approximately 18% of publications constitute foundational knowledge, while 59% represent emerging or low-citation work—typical of expanding, multi-disciplinary research domains. This stratification highlights two important characteristics of the research domain. Foundational computational and theoretical work continues to anchor academic and industrial discourse on monitoring and automation. The field has also expanded into domains where influence is reflected less in citation counts and more in regulatory adoption and institutional relevance, particularly in areas linked to governance, ability to explain decisions to auditors, risk management, and organisational change within financial institutions. Highly Influential Works and Their Significance The most influential works in the dataset can be grouped into three broad categories: methodological foundations for process monitoring, structural perspectives on use of digital technologies and governance, and workforce-focused studies on human–AI interaction. Foundational Methodological Contributions Wil van der Aalst’s Process Mining: Data Science in Action (2016) remains the single most influential publication, serving as the conceptual and technical anchor for virtually all subsequent developments in process analytics and predictive monitoring. Its thousands of citations reflect its foundational role in defining eventlog concepts, process discovery, conformance checking, and performance analysis. Structural and Organisational Foundations Several high-impact publications contribute fundamentally to understanding how organisations, especially financial institutions adopt and govern the technologies used in monitoring and control. Goodhue and Thompson’s (1995) Task-Technology Fit (TTF) theory represents a foundational model for evaluating the alignment between technological capabilities and task requirements. Its continued influence is evident across use of digital technologies, RPA adoption, and performance management studies. Workforce Transformation and Human–AI Interaction The workforce cluster is anchored by several highly cited works that illuminate behavioural, organisational, and psychological responses to automation. Venkatesh and Davis’s (2003) unified theory of technology acceptance (UTAUT) continues to dominate analyses of workforce adoption and resistance dynamics, particularly in contexts where automated systems influence decision-making and operational routines. Glikson and Woolley’s (2020) comprehensive review of human trust in AI stands as a cornerstone for understanding the cognitive and emotional dimensions of automation acceptance. Together with Hancock and Nourbakhsh’s (2020) work on anthropomorphism and trust, these studies help describe why appropriate levels of trust remains central to sustainable human–machine collaboration. Interpretation of Citation Trends Over Time Citation patterns over the period 2012–2025 reveal clear thematic transitions that mirror technological and regulatory developments in financial monitoring. Early work (2012–2016) was dominated by methodological extends in process mining, event-log cleaning, and pattern detection. This period generated several foundational works that continue to attract high and sustained citation counts. The period between 2017 and 2020 witnessed an explosion of interest in machine learning, deep learning, and predictive process monitoring. Citation patterns confirm that deep learning studies (e.g., Tax et al., 2017; Becker et al., 2014) saw a steep rise during this time, aligning with increased availability of large event logs, expanded computational capabilities, and industry interest in predictive monitoring for risk management. After 2020, citation trends reflect a shift toward concerns about explainability, governance, ability to explain decisions to auditors, and human–AI interaction. Publications focusing on XAI, model interpretability, mental effort required, and organisational adaptation show increasing citation velocity, reflecting the maturing of predictive monitoring technologies and their integration into regulated industries such as finance. Figure 4: Citation Trends Citation trends reflect the methodological and thematic evolution of the field. Foundational works (2012–2015) retain long-term influence, deep learning papers peak between 2017 and 2020, and XAI/governance work accelerates post-2020. 
3.1.2 Author Analysis Leading Authors in Each Cluster The field is shaped by several highly influential and highly interconnected author communities. In the Process Cluster, Wil van der Aalst stands as the central intellectual figure, contributing extensively to process mining, predictive monitoring, and conceptual frameworks that underpin the field. His collaborations with authors such as Marlon Dumas, Marcello La Rosa, Massimiliano de Leoni, and Niall Tax form a dense co-authorship network that dominates methodological developments. Figure 5: Process Cluster Author Analysis In the Structural Cluster, thought leadership is more distributed. Van Dongen, Thomas Davenport, Mary Lacity, Leslie Willcocks, and Peter Mikalef form the intellectual backbone of research on use of digital technologies, AI adoption, governance, and data capabilities. Their outputs frequently intersect with institutional practice, offering frameworks that guide financial institutions’ implementation strategies. Figure 6: Structural Cluster Author Analysis The Workforce Cluster is characterised by interdisciplinary authorship spanning psychology, management science, information systems, and HCI. Venkatesh, Woolley, Glikson, Hancock, and Susskind are among the most influential contributors, shaping theoretical and empirical understanding of how individuals and teams respond to automation and AI-enabled systems. Figure 7: Workforce Cluster Author Analysis Author Network Structure Network analysis reveals three distinct author communities with varying levels of connectivity. The BPM/Process Mining network is tightly interconnected, with frequent co-authorship and cross-citation. These authors collectively define the methodological landscape of predictive monitoring and related computational techniques. The Digital Governance network is more fragmented but shows significant crosspollination between academic and practitioner ecosystems. These authors often engage with regulatory institutions, consulting firms, and policy organisations, producing work that bridges theoretical understandings and institutional practice. The Human–AI Interaction network is the most interdisciplinary of the three, drawing on psychology, organisational behaviour, sociotechnical systems theory, and ethics. These authors exhibit moderate interconnectedness but strong thematic alignment around trust, acceptance, mental effort required, and collaboration. Their contributions are increasingly shaping design principles for human-in-the-loop monitoring systems in finance. 
3.1.3 Institutional Analysis Academic Institutions Shaping the Field Figure 8: Process Cluster Institutional Analysis The most influential academic institutions vary by cluster. In the Process Cluster, Eindhoven University of Technology, the University of Tartu, Queensland University of Technology, and the University of Padua emerge as global centres for process mining and predictive monitoring research. These institutions have produced the majority of the methodological works and provide the technical foundations for event-log analytics. Figure 9: Structural Cluster Institutional Analysis Figure 10: Structural Cluster Institutional Analysis 2 In the Structural Cluster, prominent institutions include MIT Sloan, the London School of Economics, Copenhagen Business School, and the University of Oslo. These institutions drive research on use of digital technologies, AI adoption, governance frameworks, and risk management in financial settings. Figure 11: Workforce Cluster Institutional Analysis Figure 12: Workforce Cluster Institutional Analysis 2 The Workforce Cluster is dominated by universities with strong HCI, psychology, and organisational behaviour programmes, such as Carnegie Mellon University, Stanford University, Harvard Business School, and the University of Michigan. Influence of Industry, Consultancy, and Regulatory Institutions Grey literature and institutional publications from regulatory bodies exert profound influence despite low or non-existent citation counts. Bodies such as the European Banking Authority (EBA), Financial Conduct Authority (FCA), Bank for International Settlements (BIS), and the International Organisation for Standardization (ISO/IEC) significantly shape the operational and regulatory landscape of financial monitoring systems. Consulting firms such as McKinsey, Deloitte, PwC, Gartner, and Celonis also play an important role. Their reports often guide industry adoption, maturity modelling, capability assessment, and investment decision-making. While not academically indexed, these outputs provide critical contextual information and industry-specific understandings that academic literature alone does not capture. 
3.1.4 Temporal Analysis The evolution of research on project monitoring, automation, and use of digital technologies in financial institutions from 2012 to 2025 reveals a clear, multi-phase progression, from conceptual foundations to advanced, specialised, and regulated automation ecosystems. Across this 13-year span, the literature shows a decisive shift in focus, methods, and practical applications, particularly as financial institutions have become early adopters and high-stakes testing grounds for automation technologies. Phase 1: Foundational Stage (2012–2015) The period from 2012 to 2015 is characterised by the establishment of fundamental theories, data practices, and analytical methods. Research during this time concentrated on process modelling, workflow optimisation, and the early conceptualisation of predictive monitoring. Much of this work was anchored in the BPM tradition, with contributions from leading scholars such as van der Aalst, Dumas, and Mendling. These studies emphasised the construction and cleaning of event logs, variant analysis, and the application of traditional machine learning techniques (e.g., decision trees, SVMs) for predicting process outcomes. In the structural domain, early research focused on data governance, IT capability frameworks, and compliance analytics, reflecting financial institutions’ concerns around regulatory reporting and digital readiness. Workforce studies, meanwhile, drew heavily on established behavioural theories such as UTAUT and technology acceptance, focusing more on digital skills and cultural readiness than on automation. Overall, this period laid the methodological and conceptual foundation for the accelerated development that followed. Figure 13: Frequency of Phase 1 (2012–2015) topics. Phase 1 research concentrated on process-level concerns (event logs, variant analysis, predictive methods) with fewer structural and workforce studies Phase 2: Acceleration and Algorithmic Growth (2016–2018) Between 2016 and 2018, the field experienced significant expansion driven by three parallel developments: i. The rise of RPA in financial institutions, ii. Improvements in enterprise data logging, and iii. The introduction of deep learning into process monitoring. This phase marks a decisive transition from conceptual discussions to real operational automation. Breakthroughs such as LSTM-based predictive process monitoring (Tax et al., 2017) provided a new algorithmic benchmark and established the viability of sequence modelling for monitoring real processes. Process mining evolved from a diagnostic technique into a predictive and prescriptive discipline. Structurally, this was the period when RPA moved from pilot experimentation to wide-scale deployment in banks, catalysed by influential practitioner-academic studies by Lacity, Willcocks, and KPMG. Infrastructure research began to explore the feasibility of scalable pipelines, data lineage, and integration risks, setting the stage for later model-risk regulation. Workforce research still focused primarily on role changes, skill shifts, and task appropriation rather than complex human–AI dynamics. This acceleration phase marks the first use of digital technologies wave, where predictive analytics and organisational automation became strategic priorities for financial institutions. Figure 14: Flow of Phase 2 research from clusters to topics to methods. The Sankey diagram illustrates the decisive expansion during 2016–2018: process research moves strongly toward LSTM and deep learning methods; structural research flows through RPA case studies and early infrastructure analysis; workforce research connects to task/skill change studies. Phase 3: Maturity, Explainability, and Governance Consolidation (2019–2021) The period from 2019 to 2021 shows a dramatic growth in publication volume across all clusters, with a clear turn toward maturation, integration, and governance. Thematically, the literature diversified, incorporating robust methodological improvements, empirical validation, and cross-sector adoption. Process research emphasised explainability, temporal reasoning, benchmarking, and robust deep learning architectures. Researchers shifted their questions from “Can we predict?” to “How can we predict reliably, transparently, and under uncertainty?” This reflects a growing awareness of real-world constraints in banking environments. This period represents the mainstream adoption of automation and predictive monitoring, supported by increasingly strict governance requirements. Figure 15: Methods used during Phase 3 (2019–2021). Phase 3 is marked by diversification: XAI tools, hybrid ML models, governance analysis, and behavioural experiments all appear prominently, reflecting the methodological maturity of the field. Figure 16: The Sankey diagram shows integrated growth across clusters: Process research flows into explainability and robust modelling; Structural research flows into governance and RegTech/SupTech adoption; Workforce research flows into trust, teaming, & reskill Phase 4: Specialisation and High-Stakes Automation (2022–2025) The most recent period shows the transition into a highly specialised, regulated, and risk-aware stage of research. Innovation is led by the combination of: i. Transformer-based models, ii. Reinforcement learning for monitoring and control, iii. Drift-resilient and real-time architectures, and iv. Cognitive and behavioural studies of human–AI collaboration. Figure 17: Phase-4 (2022–2025) topics. The period is characterised by specialised, high-stakes automation themes: transformer-based monitoring, reinforcement learning, drift resilience, international regulatory governance, and cognitive/behavioural research on supervising AIaugmented environments. Figure 18: Flow of Phase 4 research from clusters to topics to methods. Process research flows into transformers, RL, and drift resilience; structural research flows into governance frameworks (ISO/OECD/EU AI Act) and resilient pipelines; workforce research flows into mental effort required, team cognition, and oversight behaviour. 
3.1.5 Journal Analysis The journal landscape underpinning research on use of digital technologies, monitoring, and control in financial institutions is distinctly multi-disciplinary, reflecting the sociotechnical complexity of the field. Across the 2012–2025 period, publications cluster into three broad journal ecosystems that map directly onto the Process, Structural, and Workforce domains of the review: i. Technical and algorithmic outlets in information systems, data science, and computer science ii. Regulatory and governance journals, including banking, audit, and riskmanagement sources iii. Behavioural, management, and human–computer interaction journals that examine organisational and workforce implications. The distribution of publications across these three ecosystems changes significantly across the four phases of research development, demonstrating how methodological sophistication, regulatory pressure, and organisational adaptation have jointly shaped the field over time. Evolution of Journals Across the Four Phases Phase 1 (2012–2015): Foundations rooted in Information Systems and BPM outlets During the foundational period, research was concentrated overwhelmingly in Information Systems (IS) and Business Process Management (BPM) journals and conferences. Outlets such as Information Systems, Information Systems Frontiers, BPM Conference Proceedings, and Data & Knowledge Engineering published early work on event-log construction, variant analysis, conformance checking, and predictive modelling using traditional machine learning. This dominance of IS/BPM journals reflects the methodological orientation of early research, which focused on establishing theoretical and technical building blocks for process mining. Structural and workforce considerations were limited and typically appeared in more conceptual journals such as MIS Quarterly (for technology acceptance) or general IS outlets where digital readiness and governance frameworks were discussed in broad terms. Phase 2 (2016–2018): The rise of applied outlets and RPA-focused journals Between 2016 and 2018, the journal landscape widened considerably with the entrance of applied AI journals (e.g., Applied Intelligence, Decision Support Systems, Expert Systems), reflecting the sudden shift toward deep learning and LSTM-based predictive monitoring. The introduction of sequence modelling approaches produced a wave of studies that were well-suited to computational and applied ML outlets, broadening the publication ecosystem beyond core BPM. Simultaneously, RPA deployment in financial institutions attracted interest from practitioner-academic channels such as MISQ Executive, Journal of Banking Operations and Technology, and industry whitepapers from Deloitte, KPMG, and banking technology publications. These outlets began publishing empirical evaluations of automation in compliance processes, producing a structural research footprint that had been largely absent in Phase 1. Phase 3 (2019–2021): Diversification driven by explainability, governance, and empirical HAI studies Phase 3 is characterised by major diversification across publication outlets, corresponding to the broadening of themes into explainability (XAI), model-risk governance, cloud-native monitoring infrastructures, RegTech/SupTech, and empirical workforce studies examining trust, override behaviour, and human–AI teaming. Process research during this period appears mainly in Knowledge-Based Systems, Applied Intelligence, Decision Support Systems, Expert Systems, and Information Systems Journal. These journals became central publication venues for XAIenhanced predictive monitoring, temporal modelling, benchmarking frameworks, and robustness-focused architectures. Structural cluster publications were increasingly concentrated in regulatory and riskmanagement journals, including: i. Journal of Banking Regulation ii. Accounting Research and auditing journals iii. Regulation & Governance iv. BIS, EBA, OECD, and OCC technical papers Workforce research expanded dominant coverage in: i. Academy of Management Annals ii. Information & Management iii. Project Management Journal iv. HCI journals (e.g., HCI, Human Factors) This reflects the shift from acceptance and skills to appropriate levels of trust, human–AI teaming, cognitive psychology, and supervisory control. Such topics map naturally onto behavioural-science and organisational journals. Phase 4 (2022–2025): Specialisation into high-stakes, domain-specific journals The most recent phase is marked by publication in highly specialised technical, regulatory, and behavioural journals, reflecting the high-stakes environment in which monitoring and control systems now operate. Process cluster work appears prominently in: i. Journal of Big Data ii. Applied Intelligence iii. Pattern Recognition iv. Expert Systems v. Machine Learning with Applications vi. IEEE Access (for real-time architectures) These outlets reflect cutting-edge research including transformers, reinforcement learning, drift-resilient architectures, hybrid deep learning models, and advanced XAI techniques. Structural cluster research is concentrated in journals and organisational bodies tied to global regulatory governance: i. Regulation & Governance ii. Computers in Industry iii. Journal of Banking Regulation iv. OECD AI Governance v. ISO 42001 documentation series vi. EU AI Act guidance vii. BIS/IMF/ITU technical reports The focus in Phase 4 is on traceability, resilience, reliability, transparency, and ability to explain decisions to auditors, meaning structural research aligns tightly with journals and entities that influence or standardise banking risk frameworks. Workforce cluster research becomes increasingly human-centred and psychological, with journals such as: i. Information & Management ii. Human Factors iii. Cognitive Engineering and Decision Making iv. Ethics & Information Technology v. Project Management Journal These outlets examine mental effort required, AI supervision, team cognition, error calibration, and human–AI task allocation under high-risk conditions. Cross-Cluster Journal Relationships 1. Process journals → Structural journals As predictive monitoring became embedded in regulated environments, the field naturally shifted from technical IS/CS journals to regulatory and compliance journals. This transition is visible in the movement from: BPM/IS → Banking Regulation/Audit Journals → OECD/EBA/OCC reports This shows how algorithmic requirements feed directly into governance demands: explainability, fairness, lineage, and ability to explain decisions to auditors. 2. Structural journals → Workforce journals Regulatory requirements around “human-in-the-loop” design stimulated behavioural studies. Thus, the journal trajectory becomes: Regulatory journals → Human Factors / Cognitive Engineering / Management Journals This reflects how compliance mandates influence workforce responsibility, supervisory roles, and mental effort required. 3. Process → Workforce journals Extends in XAI and uncertainty quantification influence: i. trust ii. interpretability iii. override decisions iv. cognitive strain Thus, process journals increasingly cross-cite behavioural journals. Journal Impact and Influence Across the entire dataset, the most influential journals (based on citation patterns, prestige, and centrality) fall into three categories: 1. Algorithmic core journals (Process cluster) i. Knowledge-Based Systems (KBS) ii. Applied Intelligence iii. Decision Support Systems (DSS) iv. Information Systems Journal These journals anchor the methodological evolution of predictive monitoring. 2. Governance and regulatory journals (Structural cluster) i. Journal of Banking Regulation ii. Accounting Research / Auditing outlets iii. Regulation & Governance iv. IEEE Access These outlets document the formalisation of monitoring, compliance automation, and oversight frameworks. 3. Behavioural and managerial journals (Workforce cluster) i. Information & Management ii. Human Factors iii. Academy of Management Annals iv. Project Management Journal These journals shape understanding of trust, oversight, and human–AI teaming. Figure 19: Journal Distribution Across Clusters Technical IS/CS journals dominate process-monitoring research, while regulatory journals and policy reports lead the structural cluster. Workforce studies are anchored in management, behavioural science, and HCI outlets. Figure 20: Sankey of journal categories to clusters Illustrates the interdisciplinarity of the field: algorithmic work flows through technical IS journals, compliance work through regulatory outlets, and behavioural transformation through management/HCI journals. 
3.2 Research Findings of Systematic Review Building on the scientometric analysis, this section presents the qualitative findings synthesized from the 114 included studies. The review process identified three dominant thematic clusters: Process Performance, Structural Performance (Governance), and Organisational/Workforce Implications. 
3.2.1 Process Performance: Efficiency, Accuracy, and Predictive Capability The systematic review reveals that the most immediate and measurable impact of automation in financial institutions is the enhancement of process performance. Consistent findings across several literature indicate that automation technologies, specifically process automation and intelligent workflow systems are fundamentally determining how monitoring and control are conducted. Theoretical Foundations The theoretical foundations underpinning process performance in automated project monitoring and control draw primarily from Business Process Management (BPM), classical, and data-guided decision-making paradigms. BPM theory conceptualises organisational work as a collection of structured, repeatable processes that can be modelled, analysed, and optimised to improve efficiency, quality, and consistency (Dumas et al., 2018; van der Aalst, 2016). Within this tradition, performance monitoring is viewed as an analytical activity that assesses deviations from designed or expected process behaviour, enabling incremental optimisation. This theoretical extension is very important in financial institutions, where monitoring and control operate under conditions of high uncertainty, regulatory scrutiny, and systemic risk. Predictive monitoring must therefore be understood not only as a performance-enhancing technology, but as a control mechanism embedded within larger risk governance frameworks (Sani, 2021; Ceravolo et al., 2024).. Past Research Directions Early research on digital process performance in project monitoring and control, particularly between 2012 and 2018, laid the conceptual groundwork for contemporary automation practices. During this period, the literature focused primarily on documenting the introduction of business process management (BPM), workflow automation, and emerging process mining technologies. Much of this work was exploratory, aiming to understand how digitisation and structured event logs could improve visibility into organisational processes. These studies always found that digitalisation made process execution more transparent, enabling organisations to identify inefficiencies, compliance deviations, and bottlenecks more systematically (van der Aalst 2016). However, the majority of these contributions emphasised descriptive analytics rather than predictive or prescriptive capabilities. Robotic Process Automation (RPA) also became increasingly prominent in the mid2010s, particularly within financial services and shared-services environments. Studies from this period, such as those examining large-scale RPA deployments in global banks, highlighted the technology’s capacity to reduce manual workload, accelerate cycle times, and minimise basic processing errors (Lacity & Willcocks 2016; Villar & Khan 2021). Yet, early deployments tended to focus on isolated backoffice processes and repetitive rule-bound tasks, rather than integrated end-to-end project monitoring workflows. As a result, the performance contributions of automation were often limited to short-term operational improvements in narrow task categories. Current Research Directions Since 2020, research on process performance in project monitoring and control has expanded rapidly, reflecting major extends in machine learning, the widespread availability of event-log data, and the increasing regulatory importance of ability to explain decisions to auditors and model governance. One of the defining developments is how predictive process monitoring (PPM) has evolved. Contemporary studies now classify models capable of predicting remaining time, next activities, deadline violations, and deviations from normal process flows. The most comprehensive recent taxonomies highlight main problems such as feature engineering for sequential logs, model calibration, and deployment issues like concept drift (Ceravolo et al. 2024). A second major research direction involves explainable predictive process monitoring. Traditional predictive models, particularly neural networks and deep sequence models, often sacrifice interpretability for accuracy. However, recent work integrates Shapley-based feature attributions, counterfactual explanations, and attention visualisations into process-monitoring contexts. Galanti et al. (2020) demonstrate one of the earliest robust frameworks for explainable PPM, enabling users to understand which process events contributed most strongly to predicted outcomes. Follow-up research further confirms that transparency is critical for user acceptance, ability to explain decisions to auditors, and effective decision-making (Galanti 2023). Methodology Landscape The methodological landscape of process performance research is dominated by quantitative, data-intensive approaches centred on event log analysis and machine learning. Most studies employ supervised learning techniques, such as recurrent neural networks, gradient boosting, and more recently transformer-based models to predict process outcomes including delays, failures, or compliance breaches (Tax et al., 2017; Polato, 2019; Yang, 2024). Performance is typically evaluated using standard metrics such as accuracy, precision, recall, and AUC, often supplemented by time-to-prediction measures. This review contributes methodological insight by arguing for multi-dimensional evaluation frameworks that integrate technical performance with decision impact and organisational context. Process performance should be assessed not only by predictive accuracy, but by timeliness, interpretability, robustness, and the quality of interventions enabled. Such an approach is essential for evaluating automated monitoring systems in financial institutions, where decision consequences are high and regulatory accountability is critical. Key Findings Synthesis Looking across the reviewed studies reveals that process performance improvements driven by automation manifest along three interdependent dimensions: efficiency, accuracy, and predictive capability. Productivity improvements are consistently linked to the automation of data capture, aggregation, and reporting, reducing manual workload and shortening feedback cycles (van der Aalst, 2016; Dumas et al., 2018). In financial institutions, this enables near-more frequent performance information into project execution, allowing deviations to be identified earlier. Accuracy improvements arise from the consistent application of analytical rules and the ability of machine learning models to detect complex patterns across large datasets (Polato et al., 2014; Tax et al., 2017). Automated monitoring reduces subjective bias and increases consistency, particularly in environments characterised by high process complexity. However, accuracy gains are highly contingent on quality of data and process stability. New Knowledge Contribution This systematic review generates new knowledge by understanding process performance in automated project monitoring and control as an anticipatory sociotechnical capability, rather than a set of isolated efficiency or accuracy improvements. While prior studies examine efficiency, accuracy, and predictive capability independently, this review shows that these dimensions form a dynamic and interdependent control system shaped by quality of data, organisational routines, and governance constraints. A key original contribution lies in identifying anticipatory control as a distinct evolution of traditional project monitoring. Rather than merely accelerating feedback cycles, predictive monitoring enables organisations to intervene before deviations materialise, fundamentally altering the logic of control. This change in perspective extends classical BPM and control theory by integrating probabilistic foresight into performance management, a shift not explicitly theorised in existing literature. Gaps and Contradictions Despite significant progress, the literature exhibits several unresolved gaps and contradictions. One major gap concerns the translation of predictive understandings into action. Many studies demonstrate high predictive accuracy without examining whether predictions lead to better decisions or improved project outcomes. This creates a disconnect between technical performance and managerial value. A second contradiction relates to standardisation versus flexibility. Automated monitoring systems perform best in structured, repeatable processes, yet projects especially in financial institutions often involve high variability and contextual nuance. The literature does not adequately resolve how predictive systems should adapt to such complexity. Another gap concerns failure modes. Studies tend to report performance improvements while under examining scenarios where automation degrades performance due to data bias, concept drift, or organisational misalignment (Liu, 2024). This optimistic bias limits understanding of risk and resilience. Future Research Directions Future research should prioritise longitudinal and impact-focused studies that examine how predictive monitoring influences project outcomes over time. Rather than evaluating models in isolation, researchers should investigate how predictive understandings shape decision-making, risk mitigation, and organisational learning. There is a need for hybrid monitoring models that integrate algorithmic prediction with structured human judgement. Such models are very important in financial institutions, where regulatory accountability necessitates meaningful human oversight. Methodological innovation is required to develop evaluation metrics that capture decision value, not just predictive accuracy. Metrics should account for timeliness, interpretability, and intervention effectiveness. Future research should explicitly integrate process performance with governance and workforce considerations, recognising that predictive capability only delivers value when embedded within appropriate organisational and regulatory structures. 
3.2.2 Structural Performance: Governance and Algorithmic Risks While the operational benefits are evident, the structural performance cluster reveals significant challenges regarding the governance and reliability of automated systems. The review identified that the increasing reliance on automated systems has raised critical concerns around accountability and reliability. Theoretical Foundations The structural performance of automated project monitoring and control systems is grounded in theories of organisational governance, risk management, and sociotechnical systems. Governance theory provides a foundational lens for understanding how control mechanisms are designed to ensure accountability, transparency, and alignment with organisational and regulatory objectives. Within financial institutions, governance structures are particularly salient due to stringent regulatory requirements, heightened risk exposure, and the systemic implications of operational failure (Goodhue and Thompson, 1995; BIS, 2019). Risk management theory further informs this domain by conceptualising monitoring as a mechanism for identifying, assessing, and mitigating operational and compliance risks. Traditional risk frameworks in finance emphasise control, documentation, and ability to explain decisions to auditors, relying on clearly defined processes and human oversight (EBA, 2021; FCA/OCC, 2017–2023). Early automation research implicitly assumed that digital systems would enhance structural performance by standardising controls and reducing discretionary variation. Socio-technical systems theory challenges this assumption by emphasising the interdependence of technical artefacts, organisational structures, and human actors. From this perspective, automation does not simply replace manual controls but restructures governance arrangements by redistributing authority, accountability, and responsibility across humans and algorithms (De Laat, 2016; Jans, 2023). Structural performance therefore emerges not solely from technical robustness but from the alignment between automated systems and institutional governance frameworks. This review contributes new theoretical insight by reframing governance not as an external constraint imposed on automation, but as a co-evolving system that must adapt to algorithmic decision-making. Structural performance is therefore best understood as the capacity of an organisation to govern automated systems responsibly, transparently, and resiliently within a regulated environment. Past Research Directions Research on structural performance from 2012 to 2018 focused primarily on the challenges and limitations posed by legacy systems, fragmented data architectures, and incomplete integration across organisational units. During this period, scholars highlighted that digital monitoring tools, especially early process mining applications, were constrained not by algorithmic capability, but by structural barriers such as heterogeneous IT landscapes, inconsistent data schemas, and insufficient event-log completeness (van der Aalst 2016). These structural deficiencies limited the scope of monitoring and impeded the generation of accurate process understandings. Another key theme involved the difficulty of establishing data lineage and governance. Before modern data platforms matured, organisational data environments were characterised by siloed systems, manual data movement, and undocumented transformations. This created challenges for continuous auditing and monitoring initiatives, which required stable and traceable structural foundations (Jans 2022). Without reliable lineage, monitoring systems struggled to ensure ability to explain decisions to auditors, a critical requirement in financial institutions. Most studies in this era relied on case-based analysis from banking, shared services, and public-sector settings. These studies always underscored that structural constraints, rather than workforce resistance or technical immaturity, were the dominant barrier to effective monitoring automation. Current Research Directions Since 2020, structural performance research has widened significantly, driven by extends in data engineering, cloud architectures, process mining platforms, and governance frameworks. A dominant contemporary theme is the emphasis on highquality event logs as the backbone of structural performance. Modern processmonitoring studies demonstrate that the reliability of predictive and descriptive analytics hinges on well-defined case identifiers, complete timestamp data, and stable data pipelines (Ceravolo et al. 2024). Structural maturity is thus increasingly apply through quality of data metrics rather than traditional organisational charts. Another important development is the focus on governance frameworks grounded in risk from inaccurate or outdated models management principles. Financial institutions, in particular, increasingly require validated model documentation, data lineage transparency, audit trails, and built-in explainability mechanisms (Jans 2022). Structural performance research now treats governance as a structural component that shapes the reliability, stability, and ability to explain decisions to auditors of automated monitoring systems. Methodological Landscape Research on structural performance adopts a diverse methodological landscape, reflecting its interdisciplinary nature. Qualitative case studies dominate governancefocused research, particularly in studies examining regulatory compliance, ability to explain decisions to auditors, and organisational control mechanisms (Kankanhalli, 2019; Jans, 2022). These studies provide rich contextual understandings but often lack generalisability. Quantitative approaches are employed in studies assessing system reliability, compliance rates, and how well control works in practice, frequently using log analysis and audit metrics (Basak, 2022; Kim and Park, 2015). Regulatory and standards-based research draws heavily on document analysis, policy review, and comparative institutional analysis, particularly in relation to risk from inaccurate or outdated models management and AI governance frameworks (ISO/IEC, 2020– 2024; OECD, 2020). A key methodological limitation identified in this review is the fragmentation of evaluation criteria. Technical studies assess system performance, while governance studies focus on compliance and accountability, with limited integration between the two. Few studies examine how algorithmic performance interacts with governance requirements in practice. Key Findings Synthesis The synthesis of findings across the structural cluster reveals that automation has a dual effect on governance and risk. On one hand, automated monitoring systems enhance structural performance by standardising controls, improving traceability, and enabling continuous compliance monitoring (van Dongen and van der Aalst, 2015; Basak, 2022). Digital logs provide auditable records that strengthen oversight and post-hoc accountability. On the other hand, automation introduces new categories of algorithmic risk. These include model opacity, data lineage uncertainty, third-party dependency, and concept drift (Afanasiev and Kandinskaia, 2021; Ceravolo, 2024). The literature consistently highlights tensions between predictive sophistication and regulatory transparency, particularly in financial institutions subject to explainability requirements. New Knowledge Contribution This review contributes new knowledge by conceptualising structural performance as algorithmically mediated governance. Existing research typically treats governance as an external constraint imposed on automated systems. In contrast, this synthesis shows that automation embeds governance logic directly into technical artefacts, redistributing accountability, authority, and control between humans and algorithms. A central original insight is the identification of the performance–governance paradox, whereby improvements in predictive capability simultaneously increase governance complexity and risk exposure. This paradox explains why automation initiatives often generate regulatory tension despite demonstrable performance gains, a pattern that individual studies acknowledge but do not fully explain. Gaps and Contradictions Despite progress, several gaps remain in understanding structural performance. The most persistent gap concerns longitudinal evidence. Few studies follow structural changes over time, making it difficult to assess whether improvements in data pipelines, architectures, or governance are durable. This limit understanding of structural drift, model degradation, and how organisations adapt architectures to evolving business needs. Research highlights contradictions in governance outcomes. Some studies report that strong governance increases stability and ability to explain decisions to auditors, whereas others find that excessive governance introduces friction, slows adaptation, and delays model updates (Jans 2022). More research is needed to identify optimal governance intensity and the conditions under which oversight enhances or inhibits structural performance. Future Research Directions Future research should adopt more substantial designs and larger perspectives to advance structural performance scholarship. Researchers should conduct causal evaluations of structural interventions, such as integration upgrades or governance enhancements, using staggered rollouts, synthetic control, and process-level experiments. These studies could directly measure structural effects on monitoring quality. Longitudinal research is essential to evaluate structural drift, maintenance burdens, and long-term stability. Multi-year analyses would reveal how structural components (data pipelines, lineage systems, architectures) degrade or improve over time. Scholars should develop resilience-oriented structural models, testing how redundancy, anomaly detection, validation cycles, and fault-tolerance mechanisms enhance stability. Simulation and digital-twin methods can play a key role in modelling structural failures and interventions. 
3.2.3 Organisational and Workforce Implications The third cluster addresses the socio-technical dimension of automation. The findings suggest that automation is not merely a technical upgrade but a transformation that creates tensions within the workforce. Theoretical Foundations The organisational and workforce implications of automated project monitoring and control are grounded in theories of technology acceptance, organisational change, human–computer interaction, and socio-technical systems. Early theoretical perspectives, such as the Unified Theory of Acceptance and Use of Technology (UTAUT), emphasise individual perceptions of usefulness, ease of use, and social influence as determinants of technology adoption (Venkatesh and Davis, 2003). These models provide an important foundation for understanding initial responses to automation but are limited in their ability to explain long-term organisational transformation. Organisational change theory extends this perspective by framing automation as a driver of structural and cultural reconfiguration. Use of digital technologies literature highlights that technology adoption reshapes routines, power relations, and professional identities, particularly in knowledge-intensive sectors such as financial services (Kane, 2019; Davenport, 2018). Within this tradition, monitoring and control functions are understood as socially embedded practices rather than purely technical tasks. Human–AI interaction and trust theory further inform this domain by examining how users interpret, rely on, and override automated systems. Appropriate levels of trust theory suggests that both overreliance and under-reliance on automation can undermine performance, emphasising the importance of explainability and transparency (Glikson and Woolley, 2020; Hancock and Nourbakhsh, 2020). Past Research Directions Between 2012 and 2018 workforce-oriented research focused primarily on how employees adapted to early automation tools (RPA, BPM) and the organisational challenges associated with shifting from manual to semi-automated monitoring processes. Early case studies in financial services documented that task automation reduced routine workloads but often introduced new interpretive burdens, requiring workers to manage exceptions rather than perform straightforward transactions (Lacity & Willcocks 2016). This transition demanded different competencies, which many institutions did not initially anticipate. During this period, research emphasised resistance, uncertainty, and change management. Employees frequently expressed concern about job displacement, loss of autonomy, and lack of clarity regarding new monitoring roles. These socio-emotional responses were shown to influence adoption rates and overall performance outcomes. Organisations that invested in training and role clarity reported smoother transitions compared with those that simply deployed automation tools without organisational preparation. Current Research Directions Since 2020, research on workforce performance has expanded significantly, focusing on trust, mental effort required, explainability, hybrid intelligence, and organisational capability development. A major theme is the study of human–AI collaboration, how workers and algorithms jointly contribute to monitoring and control. Empirical studies show that automation changes the cognitive nature of work, shifting employees from manual detection to interpretive judgement, triage, and escalation decision-making (Glikson & Woolley 2020). This shift demands greater critical thinking, digital literacy, and comfort with uncertainty. A second major direction is research on explainability and user comprehension. Predictive monitoring systems now generate complex signals based on machine learning models. Studies such as Galanti (2023) show that explanations, such as Shapley attributions or explanation dashboards, improve comprehension, reduce override errors, and enhance decision quality. Organisations increasingly use machine-learning models explanations as training tools to help staff understand data-driven predictions. Post-2020 research also emphasises the development of organisational digital capabilities. Workforce performance improves when organisations invest in training, data literacy, AI awareness, and cross-functional collaboration routines. Studies in financial auditing show that teams with higher data competence more effectively interpret monitoring outputs, improving the speed and quality of interventions (Jans 2022). Methodological Landscape Methodologically, research on organisational and workforce implications employs a mix of quantitative surveys, qualitative case studies, and conceptual analyses. Survey-based studies frequently use acceptance and trust models to examine user attitudes toward automation, often relying on self-reported measures of trust, perceived usefulness, and intention to use (Venkatesh and Davis, 2003; Glikson and Woolley, 2020). While these approaches offer generalisable understandings, they often capture perceptions at a single point in time. Qualitative case studies provide richer understandings into how automation reshapes roles, responsibilities, and organisational routines. Studies in auditing, finance, and project management document how monitoring roles evolve in response to automated systems, emphasising shifts toward analytical and supervisory tasks (Kokina and Blanchette, 2019; Lacity and Willcocks, 2018). However, such studies are often context-specific and lack comparative breadth. Key Findings Synthesis The literature on organisational and workforce performance reveals several consistent findings about how automation reshapes monitoring, decision-making, and work practices. A first major insight is that automation shifts workforce tasks from execution to oversight, fundamentally altering skill requirements. Workers increasingly focus on interpreting alerts, assessing risk predictions, and making judgement calls rather than manually identifying anomalies (Galanti 2023). This shift often improves decision quality but simultaneously increases mental effort required. Research consistently shows that trust in automated systems is central to workforce performance. Trust that is too high leads to automation bias, where employees accept alerts without verification; trust that is too low leads to unnecessary overrides, manual checks, and delayed interventions (Glikson & Woolley 2020). The challenge is achieving calibrated trust, which requires clear explanations, historical accuracy, and predictable system behaviour. New Knowledge Contribution This review contributes new knowledge by conceptualising structural performance as algorithmically mediated governance. Existing research typically treats governance as an external constraint imposed on automated systems. In contrast, this synthesis shows that automation embeds governance logic directly into technical artefacts, redistributing accountability, authority, and control between humans and algorithms. A central original insight is the identification of the performance– governance paradox, whereby improvements in predictive capability simultaneously increase governance complexity and risk exposure. This paradox explains why automation initiatives often generate regulatory tension despite demonstrable performance gains, a pattern that individual studies acknowledge but do not fully explain. The review extends theory by framing governance as a co-evolving system, rather than a static framework. Structural performance is thus redefined as an organisation’s capacity to continuously align algorithmic capability with regulatory, ethical, and accountability requirements in dynamic environments. Gaps and Contradictions Several unresolved gaps persist in the workforce-performance literature. The most significant gap concerns longitudinal evidence. Most studies examine workforce performance shortly after automation adoption, but few track how trust, workload, and role adaptation evolve over multiple years. This limit understanding of how workforce practices stabilise or destabilise over time. There are also contradictions concerning trust. Some research reports that automation improves decision-making by reducing human error; other studies document that automation bias leads to over-reliance and reduced vigilance. Conversely, algorithm aversion can undermine effective use. These contradictory findings suggest that contextual factors, experience, training, regulatory environment, shape trust patterns in ways not yet fully understood (Glikson & Woolley 2020). Future Research Directions Future research should address the gaps identified above by advancing both behavioural inquiry and organisational analysis. Scholars should conduct longitudinal studies to track how workforce trust, mental effort required, adaptation, and role reconfiguration evolve post-automation. Such studies are crucial for understanding sustained socio-technical impacts. Future research should investigate remote and hybrid digital monitoring teams, examining how distributed collaborative structures influence performance, coordination, and resilience. Scholars should explore digital skill inequity and inclusion. Ensuring equitable workforce performance requires understanding how use of digital technologies affects different employee groups and designing interventions that reduce capability gaps. 
3.2.4 Cross-Cluster Synthesis Integrative Theoretical Perspective The cross-cluster synthesis integrates findings from the process performance, structural performance, and organisational/workforce clusters to generate a holistic understanding of how automation reshapes project monitoring and control in financial institutions. Individually, each cluster provides valuable but partial understandings: process research focuses on efficiency and predictive capability; structural research emphasises governance, risk, and compliance; workforce studies examine trust, roles, and cognitive adaptation. When synthesised, however, these strands reveal that automated monitoring functions as a deeply interconnected sociotechnical control system, rather than a collection of independent technological improvements. From a theoretical standpoint, this synthesis extends socio-technical systems theory by demonstrating that control in automated environments is distributed across algorithms, governance frameworks, and human actors. Process performance gains achieved through predictive analytics depend on governance mechanisms that legitimise and constrain algorithmic decision-making, as well as workforce capabilities that enable interpretation and intervention. Control is therefore not located in any single component, but emerges from the alignment of all three clusters. Interdependencies Between Process Performance and Structural Governance The synthesis reveals a strong interdependence between process performance improvements and structural governance arrangements. Predictive monitoring systems enhance efficiency and accuracy by enabling early detection of deviations and proactive intervention. However, these benefits are contingent on governance structures that ensure quality of data, model validity, and regulatory compliance. Studies in the process cluster frequently assume stable and reliable data infrastructures, yet structural research highlights that data lineage, integration, and external components introduce significant risks. Without robust governance, predictive accuracy may mask underlying data or model deficiencies, leading to false confidence in automated outputs. This creates a feedback loop in which perceived performance improvements can undermine structural integrity if governance mechanisms lag behind technological adoption. Human–AI Interaction as the Integrating Mechanism The workforce cluster emerges as the critical integrating mechanism linking process and structural dimensions. Human actors serve as the interface through which predictive understandings are interpreted, validated, and enacted. Appropriate levels of trust, explainability, and cognitive workload directly influence whether process performance gains translate into effective control or become sources of new risk. The synthesis reveals that workforce adaptation is not optional but foundational to automated monitoring. Even highly accurate predictive systems require human oversight to manage exceptions, ethical considerations, and regulatory accountability. Structural governance frameworks often assume rational compliance, yet workforce studies demonstrate that trust, understanding, and role clarity shape how governance is enacted in practice. Emergent System-Level Dynamics and Trade-offs Synthesising across clusters reveals several emergent system-level dynamics that are not apparent within individual research streams. One such dynamic is the performance–governance–cognition trade-off, whereby improvements in predictive sophistication increase governance complexity and cognitive demand. Highly advanced models may deliver superior forecasts but require greater explainability, validation, and human oversight. Another emergent dynamic concerns temporal misalignment. Process performance improvements often materialise faster than organisational and governance adaptation, creating periods of vulnerability where systems operate beyond existing formal control structures. This lag effect explains why automation initiatives may initially appear successful but later encounter resistance or regulatory challenges. These dynamics contribute original knowledge by explaining why automation outcomes vary across organisations, even when similar technologies are deployed. Success depends not on isolated excellence within a single cluster, but on the synchronised evolution of process, structure, and workforce capabilities. An Integrated Conceptual Model of Automated Monitoring and Control Based on the cross-cluster synthesis, this review proposes an integrated conceptual model in which automated project monitoring and control operates through three interlocking layers: predictive process analytics, governance and risk frameworks, and human oversight and intervention. Each layer both constrains and enables the others, forming a dynamic control system. This model extends existing literature by shifting attention from technology adoption to control alignment. Automation delivers sustainable value only when predictive capability is matched by ability to oversee and account for and workforce readiness. Misalignment across layers results in performance degradation, governance failure, or workforce disengagement. 
3.2.5 Connection to Research Questions RQ1: What are the current practices, processes, and systems used for project monitoring and control within financial institutions? Current practice in financial institutions combines traditional governance procedures with increasingly data-driven, continuous monitoring systems. At the process level, organisations still rely on core control routines, periodic status reporting, milestone reviews, variance analyses, and exception reporting, embedded in project management frameworks (PMO processes, stage-gates). These traditional mechanisms remain important for contractual and regulatory reporting, but they are now frequently complemented by continuous process monitoring that uses event logs and transaction traces captured by enterprise systems (ERP, core banking, loan systems) to create near-more frequent performance information (van der Aalst 2016). Process-mining platforms and business-intelligence/dashboarding tools (e.g., Splunk, Power BI, Celonis) are widely used to extract, visualise and analyse process flows, identify bottlenecks, and produce operational KPIs (cycle time, throughput, conformance). Where institutions have moved beyond descriptive dashboards, they deploy predictive process monitoring (PPM) models that forecast remaining time, likely next steps, or the probability of deadline/budget breaches; PPM pipelines typically use sequence encodings, time features, and classifiers (Ceravolo et al. 2024). Robotic Process Automation (RPA) and workflow orchestration tools are also commonly applied to automate routine exception handling and data reconciliation tasks (Lacity & Willcocks 2016). RQ2: How is automation being applied within financial institutions to support and enhance project monitoring and control activities? Automation in financial institutions supports project monitoring across three complementary vectors: data capture and integration, rule-based automation of routine tasks, and predictive/prescriptive analytics. Automation streamlines data capture: connectors, APIs, and event-streaming platforms automatically extract timestamps, case IDs and event attributes from operational systems, creating the event logs that underpin monitoring (van der Aalst 2016). Automated ETL pipelines and data-quality checks reduce manual reconciliation and improve the feed into analytic models. Rule-based automation (RPA, workflow engines) handles repetitive monitoring tasks, reconciliation, checklist verification, and exception routing, thereby reducing cycle time and human error. RPA bots can enact remediation steps (e.g., resubmitting transactions, flagging vendors) and escalate complex cases to human operators, enabling a hybrid human-automation workflow (Lacity & Willcocks 2016). RQ3: In what ways does automation influence the effectiveness, accuracy, and risk profile of project monitoring and control? Automation has multi-faceted effects: it improves effectiveness and accuracy for structured tasks but also introduces new risk modalities that must be managed. On effectiveness, automation increases observability and responsiveness (shorter lead times to detection). Predictive models can flag likely delays or cost variances earlier than periodic reviews, enabling proactive interventions that reduce downstream remediation effort (Ceravolo et al. 2024). For repetitive monitoring activities, RPA significantly reduces manual errors and cycle times, improving throughout (Lacity & Willcocks 2016). Accuracy gains are contingent on quality of data and model robustness. Where event logs are complete and well-engineered, PPM models achieve useful predictive performance; however, incomplete timestamps, inconsistent case identifiers, and fragmented data sources degrade accuracy rapidly (van der Aalst 2016). Explainability methods improve practical accuracy in deployment by helping users interpret why a model made a prediction, which increases correct follow-through and reduces inappropriate overrides (Galanti et al. 2020). RQ4: What organisational and workforce implications arise from the adoption of automation in project monitoring and control, and what strategies can support sustainable implementation? Automation reshapes roles, competencies, and organisational practices. Work content shifts from manual transaction processing to exception handling, interpretation of predictive outputs, and oversight of automated processes, increasing demand for data literacy, analytical judgement, and domain-specific model understanding. This role shift may generate anxiety about job security, require reskilling, and change career paths within PMOs and control functions (Lacity & Willcocks 2016). Human–AI interaction research shows that workforce outcomes depend on appropriate levels of trust: staff must trust automated outputs sufficiently to act when appropriate but retain critical oversight to avoid automation bias (Glikson & Woolley 2020). Explainability tools and training reduce mis calibrated trust by showing which features drive predictions and by illustrating typical failure modes, thereby improving decision quality and audit readiness (Galanti et al. 2020). 4 Discussion 
4.0 Introduction This chapter highlights the results from the systematic review and scientometric analysis. While the previous chapter focused on presenting and categorising empirical evidence, this chapter interprets those findings in relation to the research aim, objectives, and questions of the study. The discussion explains the findings within the academic literature on automation, project monitoring and control, and use of digital technologies in financial institutions. Consistent with the thematic structure established in Chapter Three, the discussion is organised around three dominant analytical dimensions: process performance, structural performance and governance, and organisational and workforce implications. By integrating several studies with established theoretical perspectives, this chapter explains how automation reshapes project monitoring and control not only as a technical function but as a socio-technical system embedded within regulatory, organisational, and human contexts. 
4.1 Automation and Process Performance in Project Monitoring and Control 
4.1.1 Reframing Process Performance under Automation This analysis indicates that automation alters the nature of process performance in project monitoring and control rather than simply improving it. Traditional monitoring frameworks emphasise retrospective measurement through periodic reporting and tracking differences from plan. Automation challenges this logic by enabling continuous data capture and near more frequent performance information, thereby shifting attention from post-hoc evaluation toward ongoing performance awareness. However, this shift should not be interpreted as an inherent performance improvement. While automated systems can accelerate information flows and reduce manual effort, the effectiveness of these gains depends on the stability and coherence of underlying project processes. In environments where processes are fragmented or frequently reconfigured, automation may increase reporting speed without improving managerial insight. Process performance therefore emerges not as a function of technological capability alone, but as an outcome shaped by organisational data maturity and process discipline. In financial institutions, this distinction is very important. Project environments are often characterised by legacy system integration, compliance-driven process change, and layered approval structures. Under such conditions, automation can surface inconsistencies rather than resolve them, exposing structural weaknesses that manual reporting previously obscured. This suggests that automation functions as a diagnostic amplifier, intensifying both strengths and deficiencies within existing monitoring practices 
4.1.2 Predictive Monitoring and the Limits of Control The evidence reviewed supports the conclusion that predictive monitoring represents a qualitative departure from traditional control models. By forecasting potential delays, cost overruns, or compliance risks, automation introduces the possibility of anticipatory intervention. Yet the practical value of this capability is frequently overstated. Predictive insight does not, in itself, constitute effective control. In financial institutions, decision-making is limited by regulatory obligations, risk formal escalation processes, and accountability requirements that limit discretionary intervention. As a result, early warnings do not consistently lead to earlier action. Instead, predictive outputs often trigger additional verification, escalation, or delay, particularly when recommendations conflict with professional judgement or regulatory caution. This analysis suggests that anticipatory control should be understood as a sociotechnical mechanism rather than a technical achievement. Automation redistributes control temporally by shifting attention earlier in the project lifecycle, but it does not eliminate uncertainty or decision risk. Where governance frameworks do not permit timely response, predictive capability remains largely symbolic, explaining why empirical findings on its effectiveness remain mixed. Accordingly, this study concludes that predictive monitoring enhances project control only under specific organisational conditions: when predictions are interpretable, when authority to act is clearly defined, and when governance structures support proactive intervention. Absent these conditions, automation may increase informational awareness without improving outcomes. 
4.1.3 Theoretical Implications for Process Performance The findings of the study contribute to theoretical understanding by extending existing perspectives on process performance in project monitoring and control. Business Process Management (BPM) theory traditionally frames performance in terms of efficiency, standardisation, and optimisation of repeatable workflows (van der Aalst, 2016). While this framework remains relevant, the findings suggest that automation introduces a qualitatively different form of performance capability centred on anticipation rather than optimisation alone. By integrating predictive analytics into monitoring practices, automation enables what can be conceptualised as anticipatory control, where performance management focuses on shaping future outcomes rather than evaluating past performance. This extends classical control theory, which is predominantly retrospective, and aligns with Dynamic Capabilities Theory by emphasising an organisation’s ability to sense emerging risks and reconfigure responses in dynamic environments (Mikalef et al., 2019). 
4.1.4 Efficiency, Accuracy and Control Automation is frequently associated with gains in efficiency and accuracy, particularly through the standardisation of data capture and reporting. This review indicates that such gains are real but limited in scope. Automated monitoring reduces manual workload and reporting latency, yet these improvements do not necessarily translate into stronger control. In practice, increased efficiency can create an illusion of control by producing more frequent and detailed metrics without improving decision quality. Where automated systems generate excessive alerts or granular performance indicators, managers may experience information overload rather than enhanced clarity. Accuracy at the data level does not guarantee relevance at the decision level, particularly when metrics are misaligned with regulatory priorities or risk thresholds. 
4.1.5 Contradictions, Failures and Conditional Performance Gains While the majority of studies report positive performance impacts from automation, the literature is not totally optimistic. Several studies document cases where automation failed to improve, or actively degraded, project monitoring and control performance. These failures are frequently attributed to poor data quality, unstable processes, and organisational over-reliance on predictive outputs without sufficient contextual judgement. For example, Liu (2024) and Ceravolo et al. (2024) show that predictive monitoring models trained on historical data often perform poorly under conditions of regulatory change, process redesign, or market volatility conditions that are common in financial institutions. In such cases, automated predictions may generate false confidence, delayed intervention or decisions. Studies highlight that predictive accuracy does not necessarily translate into improved decision-making. High-performing models may overwhelm managers with alerts or produce recommendations that lack actionable clarity, resulting in alert fatigue or decision paralysis (Ni, 2023). These findings contradict assumptions that technical performance gains automatically lead to managerial effectiveness. This evidence shows that automation’s contribution to process performance is conditional rather than deterministic. Performance improvements depend not only on algorithmic sophistication, but on data governance, process stability, and the organisation’s capacity to interpret and act on automated outputs. This challenges technologically driven narratives and reinforces the need to evaluate automation within its organisational and regulatory context. 
4.2 Structural Performance: Governance, Transparency, and Algorithmic Risk 
4.2.1 Reconfiguring Governance under Automated Control This analysis indicates that automation fundamentally alters how governance is enacted in project monitoring and control. Traditional governance mechanisms rely on clearly delineated roles, procedural oversight, and human accountability. Automation disrupts this arrangement by embedding rules embedded in systems within technical systems, redistributing responsibility across algorithms, data pipelines, and organisational actors. Rather than strengthening governance by default, automation introduces new points of fragility. When monitoring rules and escalation thresholds are encoded into automated systems, governance becomes dependent on model design, data integrity, and system configuration. These elements are often managed by technical specialists who are organisationally distant from formal accountability structures. As a result, responsibility for monitoring outcomes becomes diffused, complicating attribution when failures occur. 
4.2.2 The Performance–Governance Conflict in Automated Monitoring The evidence reviewed supports the conclusion that automation generates a performance–governance paradox. While automated monitoring can improve consistency, speed, and predictive capability, these gains often intensify governance challenges rather than resolve them. Enhanced technical performance increases reliance on systems whose decisions may be difficult to explain, audit, or contest. In financial institutions, this paradox is magnified by regulatory density and risk sensitivity. Supervisory expectations emphasise traceability, accountability, and human oversight, all of which are strained by opaque or highly automated control systems. As a result, automation initiatives that succeed operationally may still be deemed problematic from a governance perspective. This paradox helps explain why automation adoption in financial services is often cautious and incremental. Institutions face pressure to innovate while simultaneously maintaining regulatory confidence. Automation therefore becomes a negotiated compromise rather than a straightforward upgrade, with governance concerns shaping both system design and deployment. 
4.2.3 Structural Risks: Data Lineage, Performance loss, and ThirdParty Dependence Beyond transparency and accountability, automation introduces new structural risks that directly affect governance reliability. Automated monitoring systems depend on continuous data flows across multiple platforms, increasing vulnerability to data quality issues, undocumented transformations, and incomplete lineage. When data provenance is unclear, governance claims based on automated outputs become difficult to defend. Model drift / Performance Loss also shows a further governance challenge. As project environments, regulatory requirements, and organisational practices evolve, predictive models may lose validity. Without systematic review and recalibration, automation can silently degrade, producing outputs that appear authoritative but no longer reflect operational reality. Dependency on third-party platforms and vendors further complicates governance. While external solutions offer scalability and advanced capabilities, they reduce institutional control over core monitoring logic. In regulated environments, this loss of control raises questions about accountability, resilience, and compliance that extend beyond technical performance considerations. 
4.2.4 Governance Failures and Regulatory Tensions Automated monitoring systems are frequently presented as governance-enhancing tools, several studies document governance failures arising from automation adoption. These failures include undocumented model changes, unclear accountability for algorithmic decisions, and insufficient human oversight in highimpact project controls. In some cases, automation has weakened governance by obscuring responsibility rather than strengthening it (Afanasiev & Kandinskaia, 2021). Regulatory investigations cited in the literature reveal instances where institutions were unable to explain automated monitoring outcomes during audits, resulting in supervisory intervention and remediation requirements. Such cases highlight a misalignment between automation capability and governance readiness. More importantly, these governance challenges are not evenly distributed across sectors. Financial institutions operate under dense regulatory regimes that demand explainability, traceability, and ability to explain decisions to auditors. As a result, governance failure in automated monitoring carries higher institutional and systemic risk than in less regulated industries. This explains why automation initiatives that succeed technically may still be deemed unacceptable from a regulatory standpoint. 
4.3 Organisational and Workforce Implications of Automated Monitoring and Control 
4.3.1 Automation and the Redistribution of Control This analysis indicates that automation fundamentally reshapes workforce roles in project monitoring and control by redistributing, rather than eliminating, control responsibilities. Automation reduces the need for manual data collection and routine reporting, but simultaneously increases reliance on human judgement to interpret, validate, and act upon automated outputs. As a result, project professionals are repositioned from executors of monitoring tasks to supervisors of algorithmic control systems. Rather than being responsible for generating performance information, individuals become responsible for deciding whether and how to respond to automated signals. This shift increases cognitive and ethical responsibility, particularly in environments where automated outputs influence compliance, risk exposure, or regulatory reporting. In financial institutions, where project decisions can have material and supervisory consequences, this reconfiguration of responsibility is especially significant. Importantly, this transformation does not simplify work. Instead, it increases the complexity of decision-making by introducing new layers of abstraction between action and outcome. Automation therefore changes what it means to “control” a project, emphasising oversight, judgement, and escalation rather than execution. 
4.3.2 Trust, Judgement and Human-AI Interaction An issue emerging from this review is the role of trust in shaping how automated monitoring systems are used in practice. Effective control depends not on blind reliance on automation, nor on its rejection, but on calibrated trust. Where trust is misaligned, automation either dominates decision-making or is marginalised, undermining its intended benefits. This analysis suggests that trust in automated monitoring is shaped less by technical accuracy than by interpretability and accountability. When automated outputs are difficult to explain or conflict with professional intuition, users tend to delay action, seek additional confirmation, or escalate decisions unnecessarily. 
4.3.3 Skills Transformation and the Limits of Reskilling Automation in project monitoring is frequently accompanied by calls for reskilling and upskilling. While this analysis recognises the importance of developing analytical and digital competencies, it also suggests that prevailing reskilling narratives are overly simplistic. The challenge is not merely acquiring technical skills but adapting professional judgement to a context where control is partially mediated by algorithms. Project professionals are required to develop new forms of expertise, including the ability to examine automated outputs, recognise model limitations, and understand the governance implications of algorithmic recommendations. These capabilities differ fundamentally from traditional project management skills and are not easily acquired through short-term training initiatives. 
4.3.4 Resistance, Adaptation, and Organisational Tension Resistance to automated monitoring should not be interpreted as opposition to technology. This analysis indicates that resistance often reflects rational concerns about accountability, loss of professional discretion, and exposure to risk. Where automation obscures responsibility or shifts liability without corresponding authority, resistance becomes a form of risk management rather than inertia. Organisational tension arises when automation initiatives prioritise efficiency over clarity of control. Employees may comply with automated processes while informally reverting to manual checks or parallel systems to protect themselves from perceived risk. Such adaptations undermine the intended benefits of automation and introduce new coordination challenges. 
4.3.5 Workforce Disruption and Deskilling Automation is often framed as augmenting professional roles; however, the literature also documents negative workforce outcomes. Several studies report deskilling effects, where increased reliance on automated monitoring reduces opportunities for experiential learning and weakens professionals’ situational awareness. Over time, this can erode the very judgement capabilities required to supervise automated systems effectively. In financial institutions, where monitoring decisions are closely tied to risk and compliance, reduced human engagement can increase systemic vulnerability. Empirical studies indicate that employees may either over-trust automated outputs or disengage entirely when accountability boundaries are unclear (Glikson & Woolley, 2020). These findings suggest that automation can undermine human control capacity if implementation focuses narrowly on efficiency. Workforce transformation therefore represents not only a change management challenge but a control risk that must be actively mitigated. 
4.4 Critical Reflections on the Literature This review leads to a clear conclusion, which is that automation does not produce consistent or predictable improvements in project monitoring and control. Although performance gains are frequently reported, they are highly uneven and even in some cases fragile. Rather than indicating a methodological disagreement, this inconsistency reflects a deeper issue in the field, showing that automation outcomes are contingent on organisational context, governance maturity, and human oversight, yet these factors are insufficiently theorised. High-quality predictions do not in themselves improve project outcomes if they are not trusted, understood, or acted upon. In practice, automated outputs frequently compete with professional judgement, regulatory caution, and organisational incentives. This creates situations in which technically accurate insights fail to influence decisions, undermining the assumed link between automation and how well control works in practice. From a governance perspective, this review indicates that automation introduces new control vulnerabilities along with its benefits. Merging monitoring logic within algorithmic systems redistributes accountability in ways that are not always transparent or manageable. Where governance frameworks fail to evolve with technical capability, automation can weaken rather than strengthen oversight, particularly in environments subject to regulatory scrutiny. Taken together, these observations support a central conclusion that automation amplifies existing organisational conditions rather than resolving them. Where ability to oversee and account for is strong and human oversight is clearly defined, automation can support anticipatory control. Where these conditions are absent, automation can cause risk, and control loss. This explains why the literature reports both success and failure without resolving the contradictions. Future research must therefore move beyond technical performance claims and examine how automation, governance structures, and professional judgement interact over time within regulated financial environments.  5 Conclusion & Recommendations This chapter concludes the study by synthesising the key findings and reflecting on their theoretical and applied consequences. Building directly on the discussion presented in Chapter Four, it revisits the research aim and questions to demonstrate how the study has addressed its objectives. The chapter also outlines the original contributions to knowledge, offers practical recommendations for financial institutions, acknowledges the study’s limitations, and proposes directions for future research. In doing so, it provides closure to the investigation while emphasising the larger relevance of automation in project monitoring and control. 
5.1 Summary of Key Findings The principal objective of the study was to examine the influence of automation on project monitoring and control in financial institutions, with particular attention to performance, governance, and organisational implications. To achieve this aim, the study conducted a systematic review and scientometric analysis of 114 academic and institutional publications. The results suggest that automation causes a big change influence on project monitoring and control across three interconnected dimensions. At the level of process performance, automation enhances efficiency, accuracy, and predictive capability. Automated data capture, real-time dashboards, and predictive analytics shorten feedback cycles and improve the consistency of monitoring outputs. Most notably, automation enables a shift from reactive, variance-based monitoring to anticipatory control, allowing project managers to identify and reduce the potential for risks before they materialise. This represents a fundamental change in how control is exercised within project environments in financial institutions. At the level of structural performance and governance, the results indicate that automation simultaneously strengthens and complicates control structures. While automated systems improve traceability, standardisation, and compliance monitoring, they also embed decision logic within algorithmic systems that are not always transparent or easily interpretable. This creates a performance–governance paradox in which improvements in predictive capability increase the complexity of accountability, ability to explain decisions to auditors, and regulatory oversight. Structural risks such as data lineage uncertainty, performance loss, and third-party dependence further shape the risk profile of automated monitoring systems. At the level of organisational and workforce implications, the study finds that automation redistributes rather than eliminates control responsibilities. Project professionals increasingly assume supervisory and interpretive roles, overseeing automated outputs and exercising judgement in exception handling. Trust, explainability, and role clarity emerge as critical determinants of effective adoption. Resistance to automation is therefore best understood as a rational response to perceived loss of professional discretion and accountability ambiguity, rather than simple opposition to technological change. 
5.2 Research Questions Revisited The study addressed the research questions as follows: RQ1: What are the current practices, processes, and systems used for project monitoring and control within financial institutions? The review found that traditional practices are largely retrospective, manual, and fragmented, relying on periodic reporting and human interpretation. These approaches limit timeliness and predictive insight. RQ2: How is automation applied to support project monitoring and control? Automation is applied through process automation, intelligent workflows, dashboards, and predictive analytics to improve data collection, reporting speed, and risk detection. RQ3: In what ways does automation influence effectiveness, accuracy, and risk? Automation improves monitoring effectiveness and accuracy but introduces new algorithmic and governance risks, including opacity, drift, and dependency on technical infrastructures. RQ4: What organisational and workforce implications arise from automation adoption? Automation reshapes roles, increases cognitive and supervisory demands, and requires new skills and trust ways to make sure effective human–machine collaboration. 
5.3 Contribution to Knowledge This study makes three interrelated contributions to the literature on automation, project monitoring and control, and governance in financial institutions. Unlike prior work that treats in isolation, each contribution is grounded in the analysis presented in Chapter Four. 
5.3.1 Contribution 1: Extending Business Process Management Toward Anticipatory Control Prior research in Business Process Management and project control has largely treated monitoring as a retrospective activity, focused on identifying deviations after they occur. Even where digital tools are discussed, control is typically framed in terms of faster reporting or improved visibility, rather than a change in the underlying logic of control. Through the analysis in Section 4.1, this study shows that automation is consistently associated with attempts to act earlier in the project lifecycle, particularly through predictive indicators and early warnings. However, the review also shows that these capabilities do not reliably lead to proactive intervention. Instead, anticipatory control appears to be constrained by governance arrangements, decision authority, and organisational risk tolerance. Based on this synthesis, this study argues that automation shifts project monitoring toward an anticipatory orientation, but only under specific conditions. This reframes process control not as a purely technical improvement, but as a socio-technical capability that depends on how organisations are structured to act on predictive information. This matters for both theory and practice because it challenges assumptions that predictive analytics alone are sufficient to improve control. 
5.3.2 Contribution 2: Conceptualising Algorithmically Mediated Governance Much of the governance literature assumes that accountability and oversight remain primarily organisational, with automation acting as a support mechanism that improves consistency and ability to explain decisions to auditors. In this view, governance frameworks are largely stable, and technology enhances their effectiveness. The discussion in Section 4.2 suggests a more complicated picture. The review shows that automated monitoring systems embed rules embedded in systems directly into algorithms and data infrastructures, which reshapes how governance is enacted in practice. Responsibility for monitoring outcomes becomes distributed across technical systems, data quality, and human oversight, often making accountability less visible rather than clearer. From this analysis, this study introduces the idea of algorithmically mediated governance to describe how control is increasingly exercised through technical artefacts as well as formal structures. This perspective helps explain why automation initiatives in financial institutions can improve operational performance while simultaneously creating regulatory and audit challenges. Recognising this mediation is important for designing governance arrangements that remain credible in highly regulated environments. 
5.3.3 Contribution 3: Reframing Workforce Transformation as Redistribution of Control Research on automation and work often focuses on whether technology replaces human labour or creates new skill requirements. In project monitoring contexts, workforce issues are frequently discussed in terms of training, acceptance, or resistance, rather than how well control works in practice. The analysis in Section 
4.3 indicates that automation in project monitoring does not remove human involvement but changes its nature. Monitoring tasks are increasingly automated, while responsibility shifts toward interpreting outputs, deciding when to intervene, and justifying decisions under regulatory scrutiny. This places greater cognitive and accountability demands on project professionals. This study therefore reframes workforce transformation as a redistribution of control rather than a process of substitution. This matters because it explains why training efforts alone often fail to address resistance or stress. In financial institutions, where professional judgement and accountability are central, workforce dynamics directly affect whether automated control systems function as intended. 
5.4 Practical Recommendations Based on the findings, the following recommendations are proposed for financial institutions seeking to implement or expand automation in project monitoring and control. Recommendation 1: Balance Predictive Capability with Explainability Financial institutions should prioritise explainable and auditable automation solutions. Predictive accuracy should not be pursued at the expense of transparency, particularly in environments subject to regulatory scrutiny. Recommendation 2: Embed Governance by Design Governance mechanisms such as data lineage documentation, model validation, audit trails, and human override capabilities should be integrated into automated monitoring systems from the design stage rather than added retrospectively. Recommendation 3: Strengthen Workforce Capability and Appropriate levels of trust Institutions should invest in reskilling programmes that develop analytical literacy, algorithmic understanding, and supervisory decision-making skills. Clear accountability frameworks and participatory change management strategies are essential to building trust in automated systems. Recommendation 4: Monitor Structural Risk Continuously Ongoing monitoring of quality of data, performance loss, and external components should form part of project formal control structures to ensure long-term reliability and resilience. 
5.5 Limitations of the Study The findings are shaped by limitations inherent to systematic literature reviews. This is because the analysis draws exclusively on published sources, it is exposed to publication bias. Studies reporting positive or successful automation outcomes are more likely to appear in academic and professional outlets than those documenting failure, resistance, or regulatory intervention. As a result, unsuccessful or problematic implementations may be underrepresented in the available evidence. In addition, the review is limited in its ability to capture how automated project monitoring systems operate over time in organisational settings. Many of the challenges associated with automation such as trust, model drift, informal workarounds, and governance breakdowns emerge gradually and are often revealed only through longitudinal research. The findings therefore reflect reported patterns in the literature rather than the full complexity of a lived organisational experience. More broadly, as a secondary study, this research cannot establish causal relationships or directly measure the impact of automation on project outcomes. Instead, it offers interpretive insight into how automation reshapes monitoring and control practices across process, governance, and workforce dimensions. These limitations point to the value of future empirical, longitudinal, and context-sensitive research to complement and extend the patterns identified in this review. 
5.6 Directions for Future Research Future research should prioritise longitudinal and mixed-method studies that examine how automated monitoring systems influence project outcomes, governance practices, and workforce behaviour over time. More research into governance, trust, and human override behaviour in high-stakes financial environments would further advance understanding of sustainable automation. Comparative studies across regulatory environments could provide valuable insight into how institutional context shapes the governance of automated project control systems